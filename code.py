# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uHJegk3ia_2MqV5nxkQzkmvuofMailir
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.models as models
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageFont
import json
import urllib.request
import os

# 关闭 matplotlib 警告
import warnings
warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')

# 设置字体
plt.rcParams['font.family'] = 'sans-serif'

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 设定超参数
lambda_adv = 10
alpha = 1e5
beta = 1e3

# 加载 ImageNet 类别索引
url = "https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json"
with urllib.request.urlopen(url) as f:
    class_idx = json.load(f)

def get_class_label(index):
    return class_idx[str(index)][1] if str(index) in class_idx else "Unknown"

# 图像预处理
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# 读取风格图片
style_image = Image.open("photo/style2.jpg").convert("RGB")
style_tensor = transform(style_image).unsqueeze(0).to(device)

# 加载 VGG19 预训练模型
vgg_features = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features.to(device).eval()
vgg_classifier = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).to(device).eval()

style_layers = ['0', '5', '10', '19', '28']
content_layers = ['21']

def get_features(image, model, layers):
    features = {}
    x = image
    for name, layer in model._modules.items():
        x = layer(x)
        if name in layers:
            features[name] = x
    return features

def gram_matrix(tensor):
    b, c, h, w = tensor.size()
    tensor = tensor.view(b, c, h * w)
    gram = torch.bmm(tensor, tensor.transpose(1, 2))
    return gram / (c * h * w)

def content_loss(content_features, generated_features, content_layers):
    return sum(torch.mean((generated_features[layer] - content_features[layer]) ** 2) for layer in content_layers)

def style_loss(style_features, generated_features, style_layers):
    return sum(torch.mean((gram_matrix(generated_features[layer]) - gram_matrix(style_features[layer])) ** 2) for layer in style_layers)

criterion_adv = nn.CrossEntropyLoss()
def generate_adversarial_example(origin, style, feature_model, classifier, target_label, num_steps=400, lr=0.01, lambda_adv=1.0, alpha=1e5, beta=1e3):
    adv_image = origin.clone().detach().requires_grad_(True).to(device)
    optimizer = optim.Adam([adv_image], lr=lr)
    content_features = get_features(origin, feature_model, content_layers)
    style_features = get_features(style, feature_model, style_layers)
    full_vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).to(device).eval()

    for step in range(num_steps):
        optimizer.zero_grad()
        output = full_vgg(adv_image)
        loss_adv = criterion_adv(output, target_label)
        gen_features = get_features(adv_image, feature_model, style_layers + content_layers)
        loss_style = style_loss(style_features, gen_features, style_layers)
        loss_content = content_loss(content_features, gen_features, content_layers)
        total_loss = lambda_adv * loss_adv + alpha * loss_style + beta * loss_content
        total_loss.backward(retain_graph=True)
        optimizer.step()

        if step % 50 == 0:
            print(f"Step {step}/{num_steps}, Loss: {total_loss.item():.4f}")
    return adv_image.detach()

def imshow(tensor, title=None, filename=None):
    tensor = tensor.cpu().clone().detach().squeeze(0)
    unnormalize = transforms.Normalize(mean=[-2.118, -2.036, -1.804], std=[4.367, 4.464, 4.444])
    tensor = unnormalize(tensor)
    tensor = torch.clamp(tensor, 0, 1)
    npimg = tensor.permute(1, 2, 0).numpy()
    image = Image.fromarray((npimg * 255).astype(np.uint8))
    draw = ImageDraw.Draw(image)
    font = ImageFont.load_default()
    text_position = (10, 10)
    text_color = (255, 0, 0)
    if title:
        draw.text(text_position, title, fill=text_color, font=font)
    plt.imshow(image)
    plt.axis('off')
    plt.show()
    if filename:
        image.save(filename)

output_path = "output2/"
os.makedirs(output_path, exist_ok=True)
target_label = torch.tensor([439]).to(device)

for i in range(1, 11):
    image_path = f"photo/origin{i}.jpg"
    origin_image = Image.open(image_path).convert("RGB")
    origin_tensor = transform(origin_image).unsqueeze(0).to(device)

    with torch.no_grad():
        origin_output = vgg_classifier(origin_tensor)
        origin_pred_label = torch.argmax(origin_output, dim=1).item()
        origin_class_name = get_class_label(origin_pred_label)
    print(f"原始图片{i} 预测标签: {origin_pred_label} ({origin_class_name})")

    adv_generated = generate_adversarial_example(origin_tensor, style_tensor, vgg_features, vgg_classifier, target_label, num_steps=300, lr=0.08, lambda_adv=lambda_adv, alpha=alpha, beta=beta)
    final_output = vgg_classifier(adv_generated)
    predicted_label = torch.argmax(final_output, dim=1).item()
    predicted_class_name = get_class_label(predicted_label)
    print(f"对抗样本{i} 预测标签: {predicted_label} ({predicted_class_name})")
    save_path = os.path.join(output_path, f"adv_origin{i}.jpg")
    imshow(adv_generated, title=f"{predicted_label} ({predicted_class_name})", filename=save_path)